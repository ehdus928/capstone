{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dd64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification \n",
    "from catalyst.utils import set_global_seed\n",
    "from pathlib import Path\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "conn_string = \"host='168.131.30.66' dbname='mimic_iii' user='dbuser' password='jnudl1'\"\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur=conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT * FROM capstone.cap_data;\")\n",
    "result = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799138ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"row_id\",\"subject_id\",\"hadm_id\",\"intime\",\"outtime\",\"information\",'readmission']\n",
    "df = pd.DataFrame(result,columns = columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    # remove, digits, spaces\n",
    "    result = \" \".join(y.split())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def make_str(lst):\n",
    "    string = \"\"\n",
    "    for sentence in lst:\n",
    "        string += (sentence+\"\\n\")\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c3386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper around Torch Dataset to perform classification\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__( self, texts, labels, label_dict = None,\n",
    "                 max_seq_length = 100000, model_name = \"bvanaken/CORe-clinical-mortality-prediction\"):\n",
    "        self.texts = texts   # 리스트[str]\n",
    "        self.labels = labels # 정수형\n",
    "        self.label_dict = label_dict # dictionary[str] = int\n",
    "        self.max_seq_length = max_seq_length # 최대 시퀀스 길이\n",
    "        \n",
    "        if self.label_dict is None and labels is not None:\n",
    "            self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))  \n",
    "                                   \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # tokenizer 경고 X\n",
    "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        # special tokens for transformers\n",
    "        # in the simplest case a [CLS] token is added in the beginning\n",
    "        # and [SEP] token is added in the end of a piece of text\n",
    "        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    # 텍스트의 길이 반환\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # 인덱스에 해당하는 요소 반환\n",
    "    def __getitem__(self, index): #return type Mapping[str, torch.Tensor]\n",
    "        # encoding the text\n",
    "        tmp = self.texts[index]\n",
    "\n",
    "        # dictionary[`input_ids`] & dictionary[`attention_mask'] : key로 사용할 input_ids와 attn_mask\n",
    "        output_dict = self.tokenizer.encode_plus(\n",
    "            tmp,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # for Catalyst, there needs to be a key called features\n",
    "        output_dict[\"features\"] = output_dict[\"input_ids\"].squeeze(0)\n",
    "        del output_dict[\"input_ids\"]\n",
    "\n",
    "        # encoding target\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "\n",
    "        return output_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f66e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(params): # parms -> dictionary 형태\n",
    "    \"\"\"\n",
    "    A custom function that reads data from CSV files, creates PyTorch datasets and\n",
    "    data loaders. The output is provided to be easily used with Catalyst\n",
    "    :param params: a dictionary read from the config.yml file\n",
    "    \"\"\"\n",
    "    \n",
    "    # reading CSV files to Pandas dataframes\n",
    "    train_df = pd.read_csv(Path(params[\"data\"][\"path_to_data\"]) / params[\"data\"][\"train_filename\"])\n",
    "    \n",
    "    valid_df = pd.read_csv(Path(params[\"data\"][\"path_to_data\"]) / params[\"data\"][\"validation_filename\"])\n",
    "    \n",
    "    test_df = pd.read_csv(Path(params[\"data\"][\"path_to_data\"]) / params[\"data\"][\"test_filename\"])\n",
    "\n",
    "    # creating PyTorch Datasets\n",
    "    train_dataset = MIMICDataset(\n",
    "        texts=train_df[params[\"data\"][\"text_field_name\"]].values.tolist(),\n",
    "        labels=train_df[params[\"data\"][\"label_field_name\"]].values,\n",
    "        max_seq_length=params[\"model\"][\"max_seq_length\"],\n",
    "        model_name=params[\"model\"][\"model_name\"],\n",
    "    )\n",
    "\n",
    "    valid_dataset = MIMICDataset(\n",
    "        texts=valid_df[params[\"data\"][\"text_field_name\"]].values.tolist(),\n",
    "        labels=valid_df[params[\"data\"][\"label_field_name\"]].values,\n",
    "        max_seq_length=params[\"model\"][\"max_seq_length\"],\n",
    "        model_name=params[\"model\"][\"model_name\"],\n",
    "    )\n",
    "\n",
    "    test_dataset = MIMICDataset(\n",
    "        texts=test_df[params[\"data\"][\"text_field_name\"]].values.tolist(),\n",
    "        labels=test_df[params[\"data\"][\"label_field_name\"]].values,\n",
    "        max_seq_length=params[\"model\"][\"max_seq_length\"],\n",
    "        model_name=params[\"model\"][\"model_name\"],\n",
    "    )\n",
    "    \n",
    "    train_val_loaders = {\n",
    "        \"train\": DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=params[\"training\"][\"batch_size\"],\n",
    "            shuffle=False,\n",
    "        ),\n",
    "        \"valid\": DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=params[\"training\"][\"batch_size\"],\n",
    "            shuffle=False,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    test_loaders = {\n",
    "        \"test\": DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=params[\"training\"][\"batch_size\"],\n",
    "            shuffle=False,\n",
    "        )\n",
    "    }\n",
    "    return train_val_loaders, test_loaders # tuple with 2 dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified version of the same class by HuggingFace.\n",
    "    See transformers/modeling_distilbert.py in the transformers repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model_name, num_classes=2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name, num_labels=num_classes\n",
    "        )\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # propagation\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (torch.Tensor): ids of each token,\n",
    "                size ([bs, seq_length]\n",
    "            attention_mask (torch.Tensor): binary tensor, used to select\n",
    "                tokens which are used to compute attention scores\n",
    "                in the self-attention heads, size [bs, seq_length]\n",
    "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
    "                we keep the head, size: [num_heads]\n",
    "                or [num_hidden_layers x num_heads]\n",
    "        Returns:\n",
    "            PyTorch Tensor with predicted class scores\n",
    "        \"\"\"\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "\n",
    "        # taking BERTModel output\n",
    "        # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
    "        bert_output = self.model(\n",
    "            input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n",
    "        )\n",
    "        # we only need the hidden state here and don't need\n",
    "        # transformer output, so index 0\n",
    "        seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
    "        # mean pooling, i.e. getting average representation of all tokens\n",
    "        pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f470283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.callbacks import (\n",
    "    AccuracyCallback,\n",
    "    CheckpointCallback,\n",
    "    InferCallback,\n",
    "    OptimizerCallback,\n",
    ")\n",
    "from catalyst.utils import prepare_cudnn, set_global_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fdf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(project_root / \"config.yml\")) as f:\n",
    "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "\n",
    "train_val_loaders, test_loaders = read_data(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c70393",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifer(\n",
    "    pretrained_model_name=params[\"model\"][\"model_name\"],\n",
    "    num_classes=params[\"model\"][\"num_classes\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=float(params[\"training\"][\"learn_rate\"])\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415197c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "set_global_seed(params[\"general\"][\"seed\"])\n",
    "prepare_cudnn(deterministic=True)\n",
    "\n",
    "# here we specify that we pass masks to the runner. So model's forward method will be called with\n",
    "# these arguments passed to it.\n",
    "trainning = SupervisedRunner(input_key=(\"features\", \"attention_mask\"))\n",
    "\n",
    "# finally, training the model with Catalyst\n",
    "trainning.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=train_val_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=int(params[\"model\"][\"num_classes\"])),\n",
    "        OptimizerCallback(accumulation_steps=int(params[\"training\"][\"accum_steps\"])),\n",
    "    ],\n",
    "    logdir=params[\"training\"][\"log_dir\"],\n",
    "    num_epochs=int(params[\"training\"][\"num_epochs\"]),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# and running inference\n",
    "torch.cuda.empty_cache()\n",
    "trainning.infer(\n",
    "    model=model,\n",
    "    loaders=test_loaders,\n",
    "    callbacks=[\n",
    "        CheckpointCallback(\n",
    "            resume=f\"{params['training']['log_dir']}/checkpoints/best.pth\"\n",
    "        ),\n",
    "        InferCallback(),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# lastly, saving predicted scores for the test set\n",
    "predicted_scores = trainning.callbacks[0].predictions[\"logits\"]\n",
    "np.savetxt(X=predicted_scores, fname=params[\"data\"][\"path_to_test_pred_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a99a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca25fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5786118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bfc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Trainer에서 사용할 하이퍼 파라미터 지정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\n",
    "    num_train_epochs=2,              # 학습 에포크 수\n",
    "    per_device_train_batch_size=8,   # 학습에 사용할 배치 사이즈\n",
    "    per_device_eval_batch_size=16,   # 평가에 사용할 배치 사이즈\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # 학습할 모형\n",
    "    args=training_args,              # 위에서 정의한 학습 매개변수\n",
    "    train_dataset=train_dataset,     # 훈련 데이터셋\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 미세조정학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc971800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb20acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f8f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "nlp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
